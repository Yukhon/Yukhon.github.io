<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[HNN summary]]></title>
    <url>%2F2020%2F12%2F02%2FHamiltonian%20Neural%20Networks%2F</url>
    <content type="text"><![CDATA[Background knowledgeForgive my poor physics knowledge, I have to summarize something about Hamiltonian mechanics. We can represent an object state represented by the combination of its momentum space and position space coordinates. With time stamp, we can build a model to describe trajectory of object f(p, q, t) Hamiltonian mechanics is kind of representation of the state, represented as$$\frac{\partial q}{\partial t} = \frac{\partial \mathcal{H}}{p}(a)\\frac{\partial p}{\partial t} = -\frac{\partial \mathcal{H}}{q}(b)$$ where $\mathcal{H}$ is actually an representation of object energy(which is constant). That’s an efficient way for us to derive S, which is (a, b), then use S predict the next state:$$s_1 = s_0 + \int_{t_0}^{t_1} S_{\mathcal{H}} dt$$ why to introduce Hamiltonian mechanicsBecause previous methods are not based on conservation law, so error accumulates (I think it’s something like gradient explosion for RNN). Instead, Hamilton addresses the problem perfectly as it’s based on the conservation laws. featuresBefore the neural network starts, we know some physical laws and use it to adjust the architecture of neural networks, including loss function, etc. some thoughts about it With the condition of vacuum(frictionless), HNN is perfect for planet orbit prediction(maybe some restriction of pixel, there are many noise data, it’s another problem needed to tackle) Since HNN could handle multiple body system, then it would be easier when tackling the relation between stars and its satellites. the session in this paper ‘learning a Hamiltonian form pixels’ would be an good example.]]></content>
      <tags>
        <tag>potential projects for summer research;</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F11%2F10%2FNLP-revision(2)%2F</url>
    <content type="text"><![CDATA[Representationone hot vectorrepresent the word in a 0-1 vector, the length would be the the length of dictionary.Obviously, it’s too space consuming. Context-based Word RepresentationWord Co-occurrence Matrix: refer to slides P12 为什么会提出这种matrix呢， 我理解的是一旦这个词后面紧跟 的词 相似的话那就说明 这两个词至少可以用法相同 PMI(Pairwise Mutual Information): $$PMI_{i,j} = log \frac{P(w_{i,j})}{P(w_i)P(w_j)}$$就是求 联合概率 用log scale. After that, we can calculate similarity through cos(i, j) However, there still exists the matrix that has high dimensionality and sparsity problem, we use SVD to decrease dimentions. Word vector == &gt; Word2vecdistributed representations. 将 word 转化为向量，在有限的window里面预测word。 Supervised learningLinear RegressionNothing needs to pick up… I think deep neural networkactivate function: to change every ‘neural’ from linear regression to non-linear one. Normal activate function: Rectified Linear Unit(Relu) == max(0,x)]]></content>
  </entry>
  <entry>
    <title><![CDATA[NLP revision(1)]]></title>
    <url>%2F2020%2F11%2F05%2FNLP-revision%2F</url>
    <content type="text"><![CDATA[At the end of semester, I would like to pick some keys regarding the course ‘document analysis, especially for the part of NLP. For easily viewing by myself, I would mix some mandarin for conceptional explanation. Boolean Retrieval(Originally presented as Term-document incidence matrix): 0-1 matrix, row represents different document, column represents different terms. But with Bigger collections, 0-1 matrix would be extremely sparse, so we introduce inverted matrix. inverted matrix: Dictionary, the key refers to each term, to simplify, the corresponding values would be a list containing document ids including the term.(We call the list as posting) INDEXER: Construct inverted index from raw text To prepare for the process of indexer, we need to preprocess the raw data. Initial Stages of Text Processing Tokenization Cut character sequence into word tokens Deal with “John’s”, a state-of-the-art solution while space tokenizer is always not enough Regex is recommended Stop words(removal) We may omit very common words (or not) E.g. the, a, to, Normalization Map text and query term to same form You want U.S.A. and USA to match Stemming and lemmatization stemming: 把各种词的三单，时态，复数 转换为单词的原型或词根模式 lemmatization: 将同义词合并在一起 Ranked RetrievalRanked Retrieval: Boolean retrieval cares little about the order of the text. We need to consider it in NLP implementation. Or we may say Boolean Retrieval is establish on Bag-Of-Words(BOW) Assumption. 或者说 由于Boolean Retrieval 本身必须是所有的关键词都存在于文件中，它本身没有一个缓冲机制 To improve this a little bit, we mark the inverted matrix with the location the term (token) is retrieved from. (Title, Author, Published date, Body) However, the problem cannot be eradicated within the Boolean Retrieval, therefore, the ranked retrieval(RR) is introduced. Ideas of RR: We need to find some score function. When the query comes into the system, the documents would be sorted by this score function, representing the closeness between the query and documents. Variations 1: Scoring with Weighted Fields, assign location with weights and sum it up. Term frequency: Sum up the number of occurrence of all the keys in a query. CONs of TF: what about each term has different importance? How to mitigate the effect of terms that occur too often in the collection? 也就是说有可能 某个key可能在每个文件都出现过，那么它的优先级就不是那么高，就跟之前的stopwords 一样 Measure the importance: Document Frequency(DF): the number of documents in the collection that contains some desired term t To measure the importance intuitively, we introduce inverse document frequency(IDF), where$$IDF_t = log \frac{N}{DF_t}$$ Then we can combine the two methods together, becoming various TF-IDF measurement $$Score_{tf-idf}(d, q) = \sum_{i=1}^{m} tf-idf_{ti,d}$$ CONs:Score linearly increases with respect to frequency of term, which is inconsistent with the actual conditions. After a certain frequency, the absolute frequency isn’t important. Solution: We introduce the weighted term frequency(wf), see slides 27 (not the point of this revision) Drawbacks： Both scoring prefers longer documents Solution: introduce Maximum tf normalization, where $$ntf_{t, d} = \alpha + (1 - \alpha) \frac{tf_{t,d}}{tf_{max}(d)}$$ Every variants is not perfect though. Seeing the data structure of vector in IR system. We could introduce vector model (more precisely, similarity model), it could be another source of scoring. OK, that’s basically all the content about IR system, then let’s talk about the evaluation. Especially, we focus most on the efficiency For Boolean Retrieval, we split the collections into: Retrieved, not retrieved 也就是说当文件被 Boolean 处理时 系统只会提取 docs 都contain 所有keys 里面的和 剩下的 Normally, we introduce Contingency Table Row index represents ground truth Relevant Not Relevant Retrieved true positive(tp) false positive(fp) Not Retrieved false negative(fn) true negative(tn) 我的理解: 第一个true or false 是判断实际情况是否与ground truth 相符，理想情况下，好的IR system 需要将所有的groud truth docs 都给提取出来，那么与理想情况相反的就是false，一致就是true。 后面的 的positive 和negative 则是 根据retrieve or not 体现的 Some measures relevant: $$Precision = \frac{tp}{tp + fp}\Recall = \frac{tp}{tp + fn}\Accuracy = \frac{tp + tn}{tp + tn + fn + fp}$$ Accuracy is useless in the field of evaluation 因为我们在evaluate 的时候只foucs on tp then F1-measure, $$F_1 = \frac{Precision * Recall}{Precision + Recall}$$ For ranked retrieval sets : Unlike the previous example, the systemretrieved all documents in our collection. Precision &amp; Recall cannot be directlyapplied in this case. Need a metric to measure the performanceof ranked list! 就是说因为RR会retrieve所有的docs， 所以我们需要在retrieve n个 文件时统计各自的precision and recall, 这就是 Precision-Recall Curve The Curve ends when recall reaches 1; Interpolated Average Precision : 就是将所有的precision-recall点 放在图中 (无retrieved 文件顺序) 良好的system 会呈直线分布 Mean Reciprocal Rank(MRR): 所有query 在此IR system 第一个groud truth 的倒数的平均数 When IR meets the webcitation analysis web analysisAuthority: Authorities are pages that are recognized as providing significant, trustworthy, and useful information on a topic. Hubs: Hubs are index pages that provide lots of useful links to relevant content pages Hyperlink-Induced Topic Search (HITS algorithm) For a specific query Q, let the set of documentsreturned by a standard search engine (e.g. VSR) becalled the root set R. Initialize S to R. Add to S all pages pointed to by any page in R. – include potential hubs Add to S all pages that point to any page in R. – include potential authority Of course then we can eliminate some dirty webpages by some rules. We set 2 sub scores: Authority score, Hubs score. Initially, we set all the nodes score 1 in the range. then we can update ap and hp.$$a_p = \sum_{q: q\to{p}}^{} h_q\h_p = \sum_{q: p\to{q}}^{} a_q$$ then normalize them using different ways. In lecture slides, it’s defined by this: $$\sum_{p\in{S}}^{} (a_p)^2 = 1 \\sum_{p\in{S}}^{} (h_p)^2 = 1$$ After several iterations, the algorithm would converge. In practice, 20 iterations produces fairly stable results. PageRank这个算法只注重authority：只要其他文章cite 我的次数越多，我这个权威性就越好 Iterate rank-flowing process until convergence Rank equation for page p: $$R(p) = c\sum_{q:q\to{p}}^{} \frac{R(q)}{N_q}$$ $N_q$ represents the number of out-links from q(q as hubs) c is the normalization constant (sum of R(q) after updating) To avoid rank sink (some pages has no reference but citing themselves), we can suppose that the browsers would randomly ‘escape’ from the cycle, opening another pages without relevance. then, new version of rank equation would be $$R(p) = c((1-\alpha)\sum_{q:q\to{p}}^{} \frac{R(q)}{N_q} + E(p))$$ where E(p) = $\frac{\alpha}{|S|}$]]></content>
      <tags>
        <tag>course</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我就像个胡适一样]]></title>
    <url>%2F2020%2F04%2F01%2Frestarto%2F</url>
    <content type="text"><![CDATA[前几天写data.frame的时候分组遇到一个问题，先贴代码 1data_df.loc[(data_df["v3_scope"]=="UNCHANGED")&amp;(data_df["v3_privilegesRequired"]=="LOW"),"v3_privilegesRequired"]="LOW1" python panda 定位条件必须用loc,以及不宜使用太多[][]不然会报错容易错误理解]]></content>
      <tags>
        <tag>ass</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[就随便写一点]]></title>
    <url>%2F2019%2F07%2F06%2F%E9%9A%8F%E4%BE%BF%E5%86%99%2F</url>
    <content type="text"><![CDATA[大二结束，其实回顾在中国的两年，真不是很满意，无论在数据结构上，还是项目上，还是学习上，都不是自己最好的状态，其实在期末考的时候的状态，虽然没有健身，饮食也不是很规律，但从自己从下午一直坐到晚上的状态真的很充实，纵使自己可能最后考试成绩可能不是很好，但真的努力过，也就是说，如果，我将这个状态带到澳洲的话，就可能真是我的大学的一个转折点specialisation 上我不知道将来会怎么选择，但如果在ai上大家都是一个起点上的话，真的是一个机会，但，ai真是一个趋势吗？真的是吗？而且底层代码我也想试一下，真的能赚钱。。。说是自己不爱编程，但我还是觉得编程是每一个没有背景，没有经济实力的人最好的赚钱方式，无论是作为跳板，还是一生谋生的方式，如果真想编程，你需要大量的学习，不断地学习（苦笑），在没有其他的追求的情况下，保持一个学习的习惯真的是编程的意义所在。说回专业选择，哎再说吧，谁知道未来几十天会发生什么呢，如果全都中国人报的话，我也可以学习看看。注意一下自己的绩点，估计又要掉到80了，这可是对申研很不利啊，出分后得好好规划一下了今天就说这么多]]></content>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[copyright test]]></title>
    <url>%2F2019%2F06%2F18%2Fcopyright-test%2F</url>
    <content type="text"><![CDATA[这是一个版权声明的测试]]></content>
      <tags>
        <tag>测试，日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo 主题优化日志]]></title>
    <url>%2F2019%2F06%2F12%2Ffirst-blog%2F</url>
    <content type="text"><![CDATA[这篇就是个测试，闹着玩的，哈哈其实就给自己一个电子日记本看看以后能不能建两个目录，一个记录日记一个记录技术自己的技术成长过程我自己真的很懒，希望自己对博客能和健身一样坚持下去其实自己现在对健身也不是很上心以后再说吧，先看看这篇能不能上传上去 1234int main&#123; printf("hello world"); return 0;&#125; 测试失败，明天整上传代码块的正确姿势6-18 已完成 代码块测试 支持本地搜索 未完成–未找到下xml文件6-19 已完成 本地搜索 字数统计未完成 –字数不显示6-22 已完成 字数统计 大工程 评论系统 –leancloud developer 模式不开放 准备部署服务器 阿里云 背景 拉长以后总是失真且不好看 –放弃 7-11 评论系统部署测试完成，hexo 项目基本完毕，下面为阿里云，域名购买和部署]]></content>
      <tags>
        <tag>日记</tag>
      </tags>
  </entry>
</search>
