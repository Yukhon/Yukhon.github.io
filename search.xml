<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Linux environment setting up]]></title>
    <url>%2F2020%2F12%2F09%2FLinux%20setting%20up%2F</url>
    <content type="text"><![CDATA[做GNN, 可能需要jaxlib的包, 找了很久都没有 支持windows的办法, 于是就在虚拟机弄了个linux的开发环境。 (虽然jaxlib说没检测到我的GPU最后也没用上,伤心) 附几个链接给自己后来需要的话一个参考 Virtual machine setting up:B站 AV号: BV1t54y1R7F3 脚本没用上,以后可以看看他的环境怎么样miniconda:原本想装 anaconda, 出错查了查是存储给小了, 又懒得重设了装了miniconda. 参考 https://zhuanlan.zhihu.com/p/69799707 Linux jupyter notebook的安装 用vscode 远程操控文件 https://blog.csdn.net/codingpy/article/details/105803890这里 有一个bug, 可能是firefox浏览缓存或者 未设置cookie的bug, 目前是重启，到时候遇到了再说吧 还有虚拟机隐藏问题 TODO: 将虚拟机图形界面隐藏]]></content>
      <tags>
        <tag>setting up</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Parameter configuration]]></title>
    <url>%2F2020%2F12%2F07%2FParameter%20configuration%2F</url>
    <content type="text"><![CDATA[Still some explanation would be used in mandarin for understanding: background abbreviation: CFHT:Canada–France–Hawaii Telescope MAGACAM(MP): CFHT的外挂仪器, 有好多个感光照相机组成 MKAM: Mauna Kea Atmospheric Monitor, another optics system.( TODO: the relationship between MKAM and MP?) MKAM DIMM seeing: TODO: Not configured yet, I guess each system has its own IQ evaluation methods, one is MPIQ, the other is MKAM DIMM Seeing. the history of CFHT(dataset)graph LR; MP-->MP-MKAM; MP-MKAM-->MP-MKAM-Vents; graph TD; A-->B; A-->C; B-->D; C-->D; parametersMINUTE_EPOCH: TODO: No idea about it. Any thoughts? most of the interval between adjacent cells 120, sometimes 60OBSID: Record IDDOMEAZ: Dome Azimuth, 穹顶方位角DOMELOUV: Dome louvers, 穹顶百叶窗, always closed since setting recording.DOMESHUT: Dome closed or openEXPMEAS: Measurement expressions?TODO: I am not sure.FILTER: makes contribution to IQGUIRADEC: no idea, always FK5 (The Fifth Fundamental Catalogue)HSTTIME: time in HawaiiMIRRCOV: Mirror covered or not? TODO: NOT sureMPIQ: MAGACAM image qualityOBJECT:PRESSURE:RELHUMID: humiditySKYBG: Sky background TODO:SPOZPMEN= SkyProbe (orig) mean absorption, magnitudeSPOZPMED= SkyProbe (orig) median absorption, magnitudeSPOZPMOD= SkyProbe (orig) mode absorption, magnitudeSPOZPSTD= SkyProbe (orig) absorption deviation, magnitude TEA2INCH:temperature delta permutationsTEA2INEB:temperature delta permutationsTEA6FOOT:temperature delta permutationsTEALOWWS:temperature delta permutationsTEAMIRCI:~ TEAMIRCO:~ TEANRLSB:~ TEAPMCLW:~ TEAPMSPN:~ TEAPMSPS:~ TEATOPOP:~ TEATOPWS:~ TEATRNGE:~ TEATRNGW:~ TEAWTHRT:~ TODO: understanding for temperature delta permutationsTELALT:Telescope altitudeTELAZ: Telescope AzimuthTEMPERAT:temperature TESCONRM:~ TESHRSET:~ TESPIERN:~ TESPIERS:~ TESPMIRE:~ TESPMIRS:~ TESPMIRW:~ TESTSPHE:~ TESTTREM:~ TESTTRNM:~ TESTTRSE:~ TESTTRSH:~ TESTTRSL:~ TESTTRWM:~ Vent R1-R6,L1-R6, 12 overall: open or closed with 3 angles TODO: what does ‘w’ represent forWDSEEMED: Dome DIMM median seeing, arc secondsWDSEEMEN: Dome DIMM mean seeing, arc secondsWDSEEMOD: Dome DIMM mode seeing, arc secondsWDSEESTD: Dome DIMM seeing deviation, arc seconds WINDDIR: wind directionWINDSCR: wind screenWINDSPED: wind speed WMSEEMED:WMSEEMEN:WMSEEMOD:WMSEESTD: domeWallTemperatureEastdomeWallTemperatureNorthdomeWallTemperatureSouthdomeWallTemperatureWest catwalkTemperatureEast: catwalk 天桥catwalkTemperatureNorthcatwalkTemperatureSouthcatwalkTemperatureWest TODO: what does that mean?domeTopSkinTemperature1Average1Minute:domeTopSkinTemperature2Average1MinutedomeTopSkinTemperature3Average1MinutedomeTopSkinTemperature4Average1Minute preprocessing data results1 completely empty: OSBTYPE ‘SONICAN’, ‘TIMESTAMP’, ‘massdimm_seeing’, ‘massdimm_target’, ‘massdimm_airmass’, ‘massdimm_flux1’, ‘massdimm_flux2’, ‘massdimm_background’, ‘massdimm_scint1’, ‘massdimm_scint2’, ‘massdimm_strehl1’, ‘massdimm_strehl2’, ‘massdimm_alt’, ‘massdimm_az’, ‘mass_seeing’, ‘mass_profile_seeing’, ‘mass_profile_1_value’, ‘mass_profile_2_value’, ‘mass_profile_3_value’, ‘mass_profile_4_value’, ‘mass_profile_5_value’, ‘mass_profile_6_value’, ]]]></content>
      <tags>
        <tag>summer research</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HNN summary]]></title>
    <url>%2F2020%2F12%2F01%2FHamiltonian%20Neural%20Networks%2F</url>
    <content type="text"><![CDATA[Background knowledgeForgive my poor physics knowledge, I have to summarize something about Hamiltonian mechanics. We can represent an object state represented by the combination of its momentum space and position space coordinates. With time stamp, we can build a model to describe trajectory of object f(p, q, t) Hamiltonian mechanics is kind of representation of the state, represented as$$\frac{\partial q}{\partial t} = \frac{\partial \mathcal{H}}{p}(a)\\frac{\partial p}{\partial t} = -\frac{\partial \mathcal{H}}{q}(b)$$ where $\mathcal{H}$ is actually an representation of object energy(which is constant). That’s an efficient way for us to derive S, which is (a, b), then use S predict the next state:$$s_1 = s_0 + \int_{t_0}^{t_1} S_{\mathcal{H}} dt$$ why to introduce Hamiltonian mechanicsBecause previous methods are not based on conservation law, so error accumulates (I think it’s something like gradient explosion for RNN). Instead, Hamilton addresses the problem perfectly as it’s based on the conservation laws. featuresBefore the neural network starts, we know some physical laws and use it to adjust the architecture of neural networks, including loss function, etc. furthersome thoughts about it With the condition of vacuum(frictionless), HNN is perfect for planet orbit prediction(maybe some restriction of pixel, there are many noise data, it’s another problem needed to tackle) Since HNN could handle multiple body system, then it would be easier when tackling the relation between stars and its satellites. the session in this paper ‘learning a Hamiltonian form pixels’ would be an good example.]]></content>
      <tags>
        <tag>summer research</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP revision(2)]]></title>
    <url>%2F2020%2F11%2F05%2FNLP-revision(2)%2F</url>
    <content type="text"><![CDATA[Representationone hot vectorrepresent the word in a 0-1 vector, the length would be the the length of dictionary.Obviously, it’s too space consuming. Context-based Word RepresentationWord Co-occurrence Matrix: refer to slides P12 为什么会提出这种matrix呢， 我理解的是一旦这个词后面紧跟 的词 相似的话那就说明 这两个词至少可以用法相同 PMI(Pairwise Mutual Information): $$PMI_{i,j} = log \frac{P(w_{i,j})}{P(w_i)P(w_j)}$$就是求 联合概率 用log scale. After that, we can calculate similarity through cos(i, j) However, there still exists the matrix that has high dimensionality and sparsity problem, we use SVD to decrease dimentions. Word vector == &gt; Word2vecdistributed representations. 将 word 转化为向量，在有限的window里面预测word。 Supervised learningLinear RegressionNothing needs to pick up… I think deep neural networkactivate function: to change every ‘neural’ from linear regression to non-linear one. Normal activate function: Rectified Linear Unit(Relu) == max(0,x)]]></content>
      <tags>
        <tag>course</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP revision(1)]]></title>
    <url>%2F2020%2F11%2F05%2FNLP-revision%2F</url>
    <content type="text"><![CDATA[At the end of semester, I would like to pick some keys regarding the course ‘document analysis, especially for the part of NLP. For easily viewing by myself, I would mix some mandarin for conceptional explanation. Boolean Retrieval(Originally presented as Term-document incidence matrix): 0-1 matrix, row represents different document, column represents different terms. But with Bigger collections, 0-1 matrix would be extremely sparse, so we introduce inverted matrix. inverted matrix: Dictionary, the key refers to each term, to simplify, the corresponding values would be a list containing document ids including the term.(We call the list as posting) INDEXER: Construct inverted index from raw text To prepare for the process of indexer, we need to preprocess the raw data. Initial Stages of Text Processing Tokenization Cut character sequence into word tokens Deal with “John’s”, a state-of-the-art solution while space tokenizer is always not enough Regex is recommended Stop words(removal) We may omit very common words (or not) E.g. the, a, to, Normalization Map text and query term to same form You want U.S.A. and USA to match Stemming and lemmatization stemming: 把各种词的三单，时态，复数 转换为单词的原型或词根模式 lemmatization: 将同义词合并在一起 Ranked RetrievalRanked Retrieval: Boolean retrieval cares little about the order of the text. We need to consider it in NLP implementation. Or we may say Boolean Retrieval is establish on Bag-Of-Words(BOW) Assumption. 或者说 由于Boolean Retrieval 本身必须是所有的关键词都存在于文件中，它本身没有一个缓冲机制 To improve this a little bit, we mark the inverted matrix with the location the term (token) is retrieved from. (Title, Author, Published date, Body) However, the problem cannot be eradicated within the Boolean Retrieval, therefore, the ranked retrieval(RR) is introduced. Ideas of RR: We need to find some score function. When the query comes into the system, the documents would be sorted by this score function, representing the closeness between the query and documents. Variations 1: Scoring with Weighted Fields, assign location with weights and sum it up. Term frequency: Sum up the number of occurrence of all the keys in a query. CONs of TF: what about each term has different importance? How to mitigate the effect of terms that occur too often in the collection? 也就是说有可能 某个key可能在每个文件都出现过，那么它的优先级就不是那么高，就跟之前的stopwords 一样 Measure the importance: Document Frequency(DF): the number of documents in the collection that contains some desired term t To measure the importance intuitively, we introduce inverse document frequency(IDF), where$$IDF_t = log \frac{N}{DF_t}$$ Then we can combine the two methods together, becoming various TF-IDF measurement $$Score_{tf-idf}(d, q) = \sum_{i=1}^{m} tf-idf_{ti,d}$$ CONs:Score linearly increases with respect to frequency of term, which is inconsistent with the actual conditions. After a certain frequency, the absolute frequency isn’t important. Solution: We introduce the weighted term frequency(wf), see slides 27 (not the point of this revision) Drawbacks： Both scoring prefers longer documents Solution: introduce Maximum tf normalization, where $$ntf_{t, d} = \alpha + (1 - \alpha) \frac{tf_{t,d}}{tf_{max}(d)}$$ Every variants is not perfect though. Seeing the data structure of vector in IR system. We could introduce vector model (more precisely, similarity model), it could be another source of scoring. OK, that’s basically all the content about IR system, then let’s talk about the evaluation. Especially, we focus most on the efficiency For Boolean Retrieval, we split the collections into: Retrieved, not retrieved 也就是说当文件被 Boolean 处理时 系统只会提取 docs 都contain 所有keys 里面的和 剩下的 Normally, we introduce Contingency Table Row index represents ground truth Relevant Not Relevant Retrieved true positive(tp) false positive(fp) Not Retrieved false negative(fn) true negative(tn) 我的理解: 第一个true or false 是判断实际情况是否与ground truth 相符，理想情况下，好的IR system 需要将所有的groud truth docs 都给提取出来，那么与理想情况相反的就是false，一致就是true。 后面的 的positive 和negative 则是 根据retrieve or not 体现的 Some measures relevant: $$Precision = \frac{tp}{tp + fp}\Recall = \frac{tp}{tp + fn}\Accuracy = \frac{tp + tn}{tp + tn + fn + fp}$$ Accuracy is useless in the field of evaluation 因为我们在evaluate 的时候只foucs on tp then F1-measure, $$F_1 = \frac{Precision * Recall}{Precision + Recall}$$ For ranked retrieval sets : Unlike the previous example, the systemretrieved all documents in our collection. Precision &amp; Recall cannot be directlyapplied in this case. Need a metric to measure the performanceof ranked list! 就是说因为RR会retrieve所有的docs， 所以我们需要在retrieve n个 文件时统计各自的precision and recall, 这就是 Precision-Recall Curve The Curve ends when recall reaches 1; Interpolated Average Precision : 就是将所有的precision-recall点 放在图中 (无retrieved 文件顺序) 良好的system 会呈直线分布 Mean Reciprocal Rank(MRR): 所有query 在此IR system 第一个groud truth 的倒数的平均数 When IR meets the webcitation analysis web analysisAuthority: Authorities are pages that are recognized as providing significant, trustworthy, and useful information on a topic. Hubs: Hubs are index pages that provide lots of useful links to relevant content pages Hyperlink-Induced Topic Search (HITS algorithm) For a specific query Q, let the set of documentsreturned by a standard search engine (e.g. VSR) becalled the root set R. Initialize S to R. Add to S all pages pointed to by any page in R. – include potential hubs Add to S all pages that point to any page in R. – include potential authority Of course then we can eliminate some dirty webpages by some rules. We set 2 sub scores: Authority score, Hubs score. Initially, we set all the nodes score 1 in the range. then we can update ap and hp.$$a_p = \sum_{q: q\to{p}}^{} h_q\h_p = \sum_{q: p\to{q}}^{} a_q$$ then normalize them using different ways. In lecture slides, it’s defined by this: $$\sum_{p\in{S}}^{} (a_p)^2 = 1 \\sum_{p\in{S}}^{} (h_p)^2 = 1$$ After several iterations, the algorithm would converge. In practice, 20 iterations produces fairly stable results. PageRank这个算法只注重authority：只要其他文章cite 我的次数越多，我这个权威性就越好 Iterate rank-flowing process until convergence Rank equation for page p: $$R(p) = c\sum_{q:q\to{p}}^{} \frac{R(q)}{N_q}$$ $N_q$ represents the number of out-links from q(q as hubs) c is the normalization constant (sum of R(q) after updating) To avoid rank sink (some pages has no reference but citing themselves), we can suppose that the browsers would randomly ‘escape’ from the cycle, opening another pages without relevance. then, new version of rank equation would be $$R(p) = c((1-\alpha)\sum_{q:q\to{p}}^{} \frac{R(q)}{N_q} + E(p))$$ where E(p) = $\frac{\alpha}{|S|}$]]></content>
      <tags>
        <tag>course</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我就像个胡适一样]]></title>
    <url>%2F2020%2F04%2F01%2Frestarto%2F</url>
    <content type="text"><![CDATA[前几天写data.frame的时候分组遇到一个问题，先贴代码 1data_df.loc[(data_df["v3_scope"]=="UNCHANGED")&amp;(data_df["v3_privilegesRequired"]=="LOW"),"v3_privilegesRequired"]="LOW1" python panda 定位条件必须用loc,以及不宜使用太多[][]不然会报错容易错误理解]]></content>
      <tags>
        <tag>ass</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[copyright test]]></title>
    <url>%2F2019%2F06%2F18%2Fcopyright-test%2F</url>
    <content type="text"><![CDATA[这是一个版权声明的测试]]></content>
      <tags>
        <tag>测试，日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo plugin setting up]]></title>
    <url>%2F2017%2F06%2F23%2FHexo%20setting%20up%2F</url>
    <content type="text"><![CDATA[这边就直接记录 hexo各种插件的安装和功能拓展吧 hexo 支持graph作图]]></content>
      <tags>
        <tag>setting up</tag>
      </tags>
  </entry>
</search>
